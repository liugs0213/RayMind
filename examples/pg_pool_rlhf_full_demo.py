#!/usr/bin/env python3
"""
PG æ±  RLHF è§’è‰²è°ƒåº¦å®Œæ•´æ¼”ç¤º
===========================

åœºæ™¯è¯´æ˜
--------
æˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸ªå•æ± å¤šè§’è‰²çš„ RLHF è®­ç»ƒç¯å¢ƒï¼Œè§’è‰²åŒ…æ‹¬ï¼š

- Rolloutï¼šç”Ÿæˆæ ·æœ¬ï¼Œä½ä¼˜å…ˆçº§ï¼Œä½¿ç”¨åŠ¨æ€ PGï¼›
- Rewardï¼šè¯„ä¼°æ ·æœ¬ï¼Œä¸­ä¼˜å…ˆçº§ï¼Œä½¿ç”¨é¢„ç•™ PGï¼›
- Trainï¼šè®­ç»ƒæ¨¡å‹ï¼Œé«˜ä¼˜å…ˆçº§ï¼Œä½¿ç”¨é¢„ç•™ PGã€‚

ç¤ºä¾‹æ¼”ç¤ºä»¥ä¸‹èƒ½åŠ›ï¼š

1. é€šè¿‡ PG æ± é¢„ç•™ Trainï¼Reward çš„èµ„æºï¼ŒRollout ä½¿ç”¨åŠ¨æ€ PGï¼›
2. å½“ PG æ± èµ„æºä¸è¶³æ—¶ï¼Œå›é€€åˆ°ä¼ ç»ŸæŠ¢å  API é‡Šæ”¾ä½ä¼˜ä»»åŠ¡å¹¶é‡æ–°åˆ›å»ºé«˜ä¼˜ä»»åŠ¡ï¼›
3. æ‰©å®¹ Rollout è§¦å‘è‡ªåŠ¨æŠ¢å ï¼ˆ`total_preemptions > 0`ï¼‰ï¼Œè§‚å¯Ÿ PG çš„å¤ç”¨ä¸é”€æ¯ï¼›
4. è¾“å‡º PG æ± ä¸æŠ¢å ç»Ÿè®¡ï¼ŒéªŒè¯æŠ¢å è®°å½•ï¼›
5. æ¼”ç¤ºä»»åŠ¡å®Œæˆåçš„æ¸…ç†æµç¨‹ã€‚

è¿è¡Œæ–¹å¼::

    python examples/pg_pool_rlhf_full_demo.py

è¿è¡Œæ—¶ä½¿ç”¨ ``local_mode=True``ï¼Œä¾¿äºåœ¨æœ¬åœ°å¿«é€Ÿæµ‹è¯•ã€‚
"""

from __future__ import annotations

import os
# Disable uv runtime-env hook to avoid psutil permission errors on restricted macOS environments.
os.environ.setdefault("RAY_ENABLE_UV_RUN_RUNTIME_ENV", "0")
import time
import random
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Tuple

import ray

# å…è®¸ç›´æ¥å¯¼å…¥ schedulemesh æºç 
REPO_ROOT = Path(__file__).resolve().parent.parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from schedulemesh.simple import SimpleScheduler, SimpleTaskSpec
from schedulemesh.config.policy import PreemptionAggressiveness


@dataclass
class RoleSpec:
    role: str
    priority: float
    cpu: float
    memory: float
    estimated_duration: float
    count: int


BASELINE_CLUSTER_CPU = 7.0  # é»˜è®¤ç¤ºä¾‹éœ€è¦çš„æ€» CPU é‡
MIN_SCALE_FACTOR = 0.25     # æœ€å¤šç¼©åˆ° 25%ï¼Œé¿å…å‡ºç° 0 èµ„æº
scale: Optional[float] = None  # æŒ‰éœ€åˆå§‹åŒ–çš„å…¨å±€ç¼©æ”¾å› å­


def _compute_scale_factor() -> float:
    """æ ¹æ® Ray é›†ç¾¤æ€» CPU è‡ªåŠ¨ç¼©æ”¾ç¤ºä¾‹èµ„æºéœ€æ±‚."""
    try:
        cluster_resources = ray.cluster_resources()
    except Exception:
        return 1.0

    total_cpu = float(cluster_resources.get("CPU", 0.0))
    if total_cpu <= 0:
        return 1.0

    factor = total_cpu / BASELINE_CLUSTER_CPU if BASELINE_CLUSTER_CPU else 1.0
    return max(MIN_SCALE_FACTOR, min(factor, 1.0))


def scale_resource(cpu: float, memory: float) -> tuple[float, float]:
    """
    æŒ‰é›†ç¾¤è§„æ¨¡ç¼©æ”¾ CPU / å†…å­˜éœ€æ±‚ï¼Œå¹¶ç¼“å­˜ç¼©æ”¾å› å­ä¾›å…¶ä»–åœºæ™¯ä½¿ç”¨.

    Returns:
        (scaled_cpu, scaled_memory)
    """
    global scale
    if scale is None:
        scale = _compute_scale_factor()

    scaled_cpu = max(round(cpu * scale, 2), 0.25)
    scaled_memory = max(round(memory * scale, 1), 256.0)
    return scaled_cpu, scaled_memory


@ray.remote
class RLHFWorker:
    def __init__(self, name: str, labels: Dict[str, str]):
        self.name = name
        self.labels = labels
        print(f"[Worker] {name} å¯åŠ¨ï¼Œlabels={labels}")

    def run(self, task_id: str, steps: int = 10, step_duration: float = 0.1):
        total = 0.0
        for idx in range(steps):
            time.sleep(step_duration)
            total += step_duration
            if idx % max(1, steps // 3) == 0:
                print(f"[Worker:{self.name}] task={task_id} progress={idx}/{steps}")
        return {"task_id": task_id, "duration": total}

    def cancel(self, task_id: str):
        print(f"[Worker:{self.name}] ä»»åŠ¡ {task_id} è¢«æŠ¢å å–æ¶ˆ")
        return {"cancelled": task_id}


def print_pg_stats(scheduler: SimpleScheduler, pool: str, stage: str) -> None:
    stats = scheduler.pg_pool_stats(pool)
    print(
        f"\n[{stage}] PG æ± ç»Ÿè®¡: total={stats.get('total_pgs', 0)}, "
        f"high_priority={stats.get('high_priority_pgs', 0)}, "
        f"available={stats.get('available_pgs', 0)}, "
        f"allocated={stats.get('allocated_pgs', 0)}, "
        f"reuse_count={stats.get('total_reuse_count', 0)}"
    )


def print_preemption_stats(scheduler: SimpleScheduler, stage: str) -> None:
    stats = scheduler.stats()
    print(
        f"[{stage}] æŠ¢å ç»Ÿè®¡: total_preemptions={stats.get('total_preemptions', 0)}, "
        f"running_tasks={stats.get('running_tasks', 0)}"
    )


def submit_role_tasks(
    scheduler: SimpleScheduler,
    pool: str,
    spec: RoleSpec,
    prefix: str,
    *,
    fallback_to_legacy: bool = False,
) -> list[str]:
    task_ids: list[str] = []
    for idx in range(spec.count):
        task_id = f"{prefix}-{idx:02d}"
        result = scheduler.submit_with_pg_preemption(
            task_id=task_id,
            pool=pool,
            actor_class=RLHFWorker,
            resources={"cpu": spec.cpu, "memory": spec.memory},
            priority=spec.priority,
            labels={"role": spec.role, "tier": prefix, "run": "rlhf-demo"},
            estimated_duration=spec.estimated_duration,
            ray_options={"name": f"{prefix}-worker-{idx:02d}"},
        )
        success = result.get("success", False)
        if not success and fallback_to_legacy:
            print(
                f"  âš ï¸  {task_id} PG æäº¤å¤±è´¥: {result.get('error', 'unknown')}. "
                "æ”¹ç”¨ä¼ ç»ŸæŠ¢å è·¯å¾„ã€‚"
            )
            legacy_spec = SimpleTaskSpec(
                task_id=task_id,
                pool=pool,
                actor_class=RLHFWorker,
                resources={"cpu": spec.cpu, "memory": spec.memory},
                priority=spec.priority,
                labels={"role": spec.role, "tier": prefix, "run": "rlhf-demo"},
                estimated_duration=spec.estimated_duration,
                auto_register=True,
                ray_options={"name": f"{prefix}-worker-{idx:02d}"},
            )
            result = scheduler.submit_spec(legacy_spec)
            success = result.get("success", False)
            print(f"  â–¶ï¸  {task_id} ä¼ ç»ŸæŠ¢å æäº¤: success={success}")
        else:
            print(f"  æäº¤ {task_id}: success={success}")
        task_ids.append(task_id)
    return task_ids


def main() -> None:
    # å¯åŠ¨ Rayï¼ˆå¦‚å·²æœ‰é›†ç¾¤åˆ™è¿æ¥ï¼‰
    if ray.is_initialized():
        ray.shutdown()
    init_attempts: list[tuple[str, dict]] = []

    ray_address = os.environ.get("RAY_ADDRESS")
    if ray_address:
        init_attempts.append(
            (f"[Ray] ä½¿ç”¨ç¯å¢ƒå˜é‡ RAY_ADDRESS={ray_address} è¿æ¥é›†ç¾¤", {"address": ray_address})
        )

    init_attempts.append(
        ("[Ray] å°è¯•è‡ªåŠ¨å‘ç°é›†ç¾¤ (address='auto')", {"address": "auto"})
    )

    for message, kwargs in init_attempts:
        try:
            print(message)
            ray.init(ignore_reinit_error=True, **kwargs)
            break
        except Exception as exc:
            print(f"{message} å¤±è´¥: {exc}")
    else:
        # æ‰€æœ‰é›†ç¾¤è¿æ¥å°è¯•å¤±è´¥ï¼Œé€€å›æœ¬åœ°æ¨¡å¼
        print("[Ray] æœªå‘ç°å¯ç”¨é›†ç¾¤ï¼Œå›é€€åˆ°æœ¬åœ°æ¨¡å¼ (local_mode=True)")
        ray.init(address="local", local_mode=True, ignore_reinit_error=True, num_cpus=8)

    scheduler = SimpleScheduler("pg-rlhf-full-demo")
    pool_name = "rlhf-pg-pool"

    # é…ç½®æŠ¢å ç­–ç•¥ï¼šTrain > Reward > Rollout
    scheduler.configure_preemption(
        enable_label_preemption=True,
        label_preemption_rules={
            "role": {
                "train": ["reward", "rollout"],
                "reward": ["rollout"],
            }
        },
        preemption_aggressiveness=PreemptionAggressiveness.MEDIUM,
        enable_cross_pool_preemption=False,
    )

    # åˆ›å»ºå¯ç”¨ PG çš„èµ„æºæ± ï¼šé¢„ç•™ Train/Rewardï¼ŒRollout ä½¿ç”¨åŠ¨æ€ PG
    scheduler.ensure_pool(
        name=pool_name,
        labels={"workload": "rlhf"},
        resources={"cpu": 6.0, "memory": 8192.0},  # æ± æ€»é…é¢
        pg_pool_config={
            "enable": True,
            "high_priority_pg_specs": [
                {"cpu": 2.0, "memory": 2048.0},  # Train é¢„ç•™ PG
                {"cpu": 2.0, "memory": 2048.0},  # Reward é¢„ç•™ PG
            ],
            "enable_dynamic_pgs": True,
            "max_dynamic_pgs": 2,  # Rollout æœ€å¤šä¸¤ä¸ªåŠ¨æ€ PG
            "enable_pg_reuse": True,
        },
    )

    print("\n=== åœºæ™¯ 1: Rollout è§’è‰²å¯åŠ¨ï¼ˆåŠ¨æ€ PGï¼‰ ===")
    rollout_spec = RoleSpec(role="rollout", priority=3.0, cpu=1.5, memory=1024.0, estimated_duration=45.0, count=2)
    rollout_tasks = submit_role_tasks(scheduler, pool_name, rollout_spec, "rollout")
    print_pg_stats(scheduler, pool_name, "Rollout å¯åŠ¨å")
    print_preemption_stats(scheduler, "Rollout å¯åŠ¨å")

    time.sleep(1.0)

    print("\n=== åœºæ™¯ 2: Reward è§’è‰²åˆ°è¾¾ï¼ˆé«˜ä¼˜è¿è¡Œï¼‰ ===")
    # ä¼˜å…ˆä½¿ç”¨é¢„ç•™ PGï¼šSimple PG æ± é€»è¾‘çº¦å®š priority >= 8.0 è§¦å‘é«˜ä¼˜ PG
    reward_spec = RoleSpec(role="reward", priority=8.5, cpu=2.0, memory=2048.0, estimated_duration=30.0, count=1)
    reward_tasks = submit_role_tasks(
        scheduler, pool_name, reward_spec, "reward", fallback_to_legacy=True
    )
    print_pg_stats(scheduler, pool_name, "Reward å¯åŠ¨å")
    print_preemption_stats(scheduler, "Reward å¯åŠ¨å")

    time.sleep(1.0)

    print("\n=== åœºæ™¯ 3: Train è§’è‰²åˆ°è¾¾ï¼ˆé¢„ç•™ PGï¼‰ ===")
    train_spec = RoleSpec(role="train", priority=9.0, cpu=2.0, memory=2048.0, estimated_duration=25.0, count=1)
    train_tasks: list[str] = []
    # å…ˆå°è¯• PG æäº¤ï¼›å¦‚æœæ± é…é¢ä¸è¶³ï¼Œä¼šèµ°å›é€€é€»è¾‘
    pg_train_result = scheduler.submit_with_pg_preemption(
        task_id="train-00",
        pool=pool_name,
        actor_class=RLHFWorker,
        resources={"cpu": train_spec.cpu, "memory": train_spec.memory},
        priority=train_spec.priority,
        labels={"role": train_spec.role, "tier": "train", "run": "rlhf-demo"},
        estimated_duration=train_spec.estimated_duration,
        ray_options={"name": "train-worker-00"},
    )
    print(f"  æäº¤ train-00 (PG ä¼˜å…ˆ): success={pg_train_result.get('success', False)}")
    if pg_train_result.get("success"):
        train_tasks.append("train-00")
        train_handle = pg_train_result["agent"]["handle"]
        print("  â–¶ï¸ è§¦å‘ train-00 worker.run()ï¼ˆPG æˆåŠŸåˆ†é…ï¼‰")
        ray.get(train_handle.run.remote("train-00", steps=20, step_duration=0.05))
    else:
        print("  âš ï¸ PG è·¯å¾„å¤±è´¥ï¼Œæ”¹ç”¨ä¼ ç»ŸæŠ¢å  API è§¦å‘è‡ªåŠ¨æŠ¢å ã€‚")
        # ä¼ ç»ŸæŠ¢å ä¼šå…ˆæŠ¢å ä½ä¼˜ä»»åŠ¡ï¼Œå†é‡æ–°åˆ›å»ºé«˜ä¼˜ä»»åŠ¡
        train_spec_legacy = SimpleTaskSpec(
            task_id="train-00",
            pool=pool_name,
            actor_class=RLHFWorker,
            resources={"cpu": train_spec.cpu, "memory": train_spec.memory},
            priority=train_spec.priority,
            labels={"role": train_spec.role, "tier": "train", "run": "rlhf-demo"},
            estimated_duration=train_spec.estimated_duration,
            auto_register=True,
            ray_options={"name": "train-worker-00"},
        )
        pg_train_result = scheduler.submit_spec(train_spec_legacy)
        print(f"  ä¼ ç»ŸæŠ¢å æäº¤ train-00: success={pg_train_result.get('success', False)}")
        if pg_train_result.get("success"):
            train_tasks.append("train-00")
            fallback_handle = pg_train_result["agent"]["handle"]
            print("  â–¶ï¸ è§¦å‘ train-00 worker.run()ï¼ˆä¼ ç»ŸæŠ¢å æˆåŠŸï¼‰")
            ray.get(fallback_handle.run.remote("train-00", steps=20, step_duration=0.05))

    print_pg_stats(scheduler, pool_name, "Train æäº¤å")
    print_preemption_stats(scheduler, "Train æäº¤å")

    print("\n=== åœºæ™¯ 4: Rollout æ‰©å®¹è§¦å‘æŠ¢å  ===")
    extra_cpu, extra_mem = scale_resource(1.5, 1024.0)
    extra_rollout = RoleSpec(role="rollout", priority=4.5, cpu=extra_cpu, memory=extra_mem, estimated_duration=20.0 * scale, count=1)
    extra_tasks: list[str] = []
    # ç›´æ¥ä½¿ç”¨ä¼ ç»ŸæŠ¢å  APIï¼Œè§‚å¯Ÿé¢å¤– Rollout è§¦å‘è‡ªåŠ¨æŠ¢å çš„è¿‡ç¨‹
    extra_spec = SimpleTaskSpec(
        task_id="rollout-extra-00",
        pool=pool_name,
        actor_class=RLHFWorker,
        resources={"cpu": extra_rollout.cpu, "memory": extra_rollout.memory},
        priority=extra_rollout.priority,
        labels={"role": extra_rollout.role, "tier": "rollout-extra", "run": "rlhf-demo"},
        estimated_duration=extra_rollout.estimated_duration,
        auto_register=True,
        ray_options={"name": "rollout-extra-worker-00"},
    )
    extra_result = scheduler.submit_spec(extra_spec)
    print(f"  æäº¤ rollout-extra-00 (ä¼ ç»ŸæŠ¢å ): success={extra_result.get('success', False)}")
    if extra_result.get("success"):
        extra_tasks.append("rollout-extra-00")
        rollout_extra_handle = extra_result["agent"]["handle"]
        print("  â–¶ï¸ è§¦å‘ rollout-extra-00 worker.run()")
        ray.get(rollout_extra_handle.run.remote("rollout-extra-00", steps=15, step_duration=0.05))

    print_pg_stats(scheduler, pool_name, "æ‰©å®¹å")
    print_preemption_stats(scheduler, "æ‰©å®¹å")

    print("\n=== åœºæ™¯ 5: æ±‡æ€»ç»Ÿè®¡ ===")
    print_pg_stats(scheduler, pool_name, "æœ€ç»ˆ")
    print_preemption_stats(scheduler, "æœ€ç»ˆ")
    print("\nğŸ“ˆ æŒ‡æ ‡è§‚å¯Ÿ: curl 127.0.0.1:8080/metrics | rg schedulemesh_ å¯æŸ¥çœ‹å®æ—¶æŠ¢å /è°ƒåº¦æŒ‡æ ‡")
    print("   å…³æ³¨ schedulemesh_preemption_count, schedulemesh_preemption_execution_latency_ms ç­‰æŒ‡æ ‡äº†è§£ PG æ•ˆæœã€‚")

    print("\n=== åœºæ™¯ 6: æ¸…ç†ä»»åŠ¡ ===")
    all_tasks = rollout_tasks + reward_tasks + train_tasks + extra_tasks
    random.shuffle(all_tasks)
    for task_id in all_tasks:
        scheduler.complete(task_id)
        deletion = scheduler.scheduler.delete_agent(task_id, force=True, destroy_pg=True)
        print(f"  æ¸…ç† {task_id}: destroy_pg=True, success={deletion.get('success', False)}")
    scheduler.shutdown()
    ray.shutdown()
    print("æ¸…ç†å®Œæˆã€‚")


if __name__ == "__main__":
    main()
